Вывод по заданию
В ходе эксперимента были протестированы три модели разного масштаба (2B, 8B и 72B параметров) на математической задаче.

google/gemma-2-2b-it (Small):

Скорость: Самая быстрая (1.57s).

Поведение: Модель "схитрила". Она верно составила уравнение, но отказалась его решать сама, предложив использовать компьютер, и просто выдумала ответ ($n=11$).

Эффективность: Высокая скорость, но низкая автономность в расчетах.

meta-llama/Meta-Llama-3-8B-Instruct (Medium):

Скорость: Аномально медленная (16.96s).

Поведение: Модель ушла в "аналитический паралич". Потратила в 6 раз больше токенов (1327), пытаясь решить задачу методом грубого перебора чисел, при этом совершая арифметические ошибки в сложении.

Эффективность: Низкая. Модель запутала сама себя излишней детализацией.

Qwen/Qwen2.5-72B-Instruct (Large):

Скорость: Неожиданно быстрая (2.39s), несмотря на огромный размер (вероятно, работает на мощном кластере GPU).

Поведение: Дала самый короткий и уверенный ответ без рассуждений.

Эффективность: Максимальная краткость, но ответ ($n=7$) неверен, как и у остальных.

Общий итог:
Больше параметров не всегда означает "умнее" в математике. LLM (языковые модели) плохо справляются с арифметикой без внешних инструментов (Python).

Маленькая модель (Gemma) оказалась самой рациональной, предложив алгоритм.

Средняя (Llama) продемонстрировала типичную проблему "галлюцинаций в расчетах".

Большая (Qwen) показала лучшую скорость генерации токенов (tokens/sec), благодаря мощной инфраструктуре Hugging Face.
